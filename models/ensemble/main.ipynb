{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import lib_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "en_model_prefix = \"en_multi30k_word\"\n",
    "de_model_prefix = \"de_multi30k_word\"\n",
    "\n",
    "en_sp = spm.SentencePieceProcessor()\n",
    "de_sp = spm.SentencePieceProcessor()\n",
    "\n",
    "en_sp.Load(f\"{en_model_prefix}.model\")\n",
    "de_sp.Load(f\"{de_model_prefix}.model\")\n",
    "\n",
    "word_vec_size = 512\n",
    "en_vocab_size = en_sp.GetPieceSize()\n",
    "de_vocab_size = de_sp.GetPieceSize()\n",
    "en_word_padding_idx = en_sp.pad_id()\n",
    "de_word_padding_idx = de_sp.pad_id()\n",
    "dropout = 0.1\n",
    "position_encoding = True\n",
    "\n",
    "def numericalize(text, tokenizer):\n",
    "    ids = tokenizer.EncodeAsIds(text)\n",
    "    print(tokenizer.DecodeIds(ids))\n",
    "    return torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good morning my friends\n",
      "tensor([1223, 2034, 1910,  482])\n",
      "▁is\n"
     ]
    }
   ],
   "source": [
    "t = \"good morning my friends\"\n",
    "print(numericalize(t, en_sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁vampire\n",
      "▁vandalized\n",
      "▁vans\n",
      "▁vaporized\n",
      "▁vat\n",
      "▁vatican\n",
      "▁vaulter\n",
      "▁vaults\n",
      "▁vegetated\n",
      "▁velodrome\n"
     ]
    }
   ],
   "source": [
    "for i in range(9980, 9990):\n",
    "    print(en_sp.IdToPiece(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.en_embeddings = lib_seq2seq.Embeddings(\n",
    "            word_vec_size,\n",
    "            en_vocab_size,\n",
    "            en_word_padding_idx,\n",
    "            position_encoding=position_encoding,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.de_embeddings = lib_seq2seq.Embeddings(\n",
    "            word_vec_size,\n",
    "            de_vocab_size,\n",
    "            de_word_padding_idx,\n",
    "            position_encoding=position_encoding,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.en_encoder = lib_seq2seq.CNNEncoder(\n",
    "            cnn_kernel_width=3,\n",
    "            num_layers=20,\n",
    "            hidden_size=512,\n",
    "            dropout=0.1,\n",
    "            embeddings=self.en_embeddings,\n",
    "        )\n",
    "        self.de_decoder = lib_seq2seq.CNNDecoder(\n",
    "            num_layers=20,\n",
    "            hidden_size=512,\n",
    "            attn_type=\"general\",\n",
    "            copy_attn=False,\n",
    "            cnn_kernel_width=3,\n",
    "            dropout=0.1,\n",
    "            embeddings=self.de_embeddings,\n",
    "            copy_attn_type=\"general\",\n",
    "        )\n",
    "        self.output_layer_de = nn.Linear(512, de_vocab_size)\n",
    "        for embedding in self.en_embeddings.emb_luts:class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.en_embeddings = lib_seq2seq.Embeddings(\n",
    "            word_vec_size,\n",
    "            en_vocab_size,\n",
    "            en_word_padding_idx,\n",
    "            position_encoding=position_encoding,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.de_embeddings = lib_seq2seq.Embeddings(\n",
    "            word_vec_size,\n",
    "            de_vocab_size,\n",
    "            de_word_padding_idx,\n",
    "            position_encoding=position_encoding,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.en_encoder = lib_seq2seq.CNNEncoder(\n",
    "            cnn_kernel_width=3,\n",
    "            num_layers=20,\n",
    "            hidden_size=512,\n",
    "            dropout=0.1,\n",
    "            embeddings=self.en_embeddings,\n",
    "        )\n",
    "        self.de_decoder = lib_seq2seq.CNNDecoder(\n",
    "            num_layers=20,\n",
    "            hidden_size=512,\n",
    "            attn_type=\"general\",\n",
    "            copy_attn=False,\n",
    "            cnn_kernel_width=3,\n",
    "            dropout=0.1,\n",
    "            embeddings=self.de_embeddings,\n",
    "            copy_attn_type=\"general\",\n",
    "        )\n",
    "        self.output_layer_de = nn.Linear(512, de_vocab_size)\n",
    "        for embedding in self.en_embeddings.emb_luts:\n",
    "            init.xavier_uniform_(embedding.weight)\n",
    "        for embedding in self.de_embeddings.emb_luts:\n",
    "            init.xavier_uniform_(embedding.weight)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=de_word_padding_idx)\n",
    "            init.xavier_uniform_(embedding.weight)\n",
    "        for embedding in self.de_embeddings.emb_luts:\n",
    "            init.xavier_uniform_(embedding.weight)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=de_word_padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_en_to_de(\n",
    "    model: Model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    en_sp,\n",
    "    de_sp,\n",
    "    num_epochs=10,\n",
    "    batch_size=32,\n",
    "):\n",
    "    def load_data(en_filepath, de_filepath):\n",
    "        en_data = []\n",
    "        de_data = []\n",
    "        with open(en_filepath, \"r\", encoding=\"utf-8\") as en_f, open(\n",
    "            de_filepath, \"r\", encoding=\"utf-8\"\n",
    "        ) as de_f:\n",
    "            for en_line, de_line in zip(en_f, de_f):\n",
    "                en_data.append(en_line.strip())\n",
    "                de_data.append(de_line.strip())\n",
    "        return en_data, de_data\n",
    "\n",
    "    en_train, de_train = load_data(\"multi30k_train_en.txt\", \"multi30k_train_de.txt\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        data = list(zip(en_train, de_train))\n",
    "        random.shuffle(data)\n",
    "        en_train, de_train = zip(*data)\n",
    "\n",
    "        # losses = [] # DEBUG\n",
    "\n",
    "        for i in range(0, len(en_train), batch_size):\n",
    "            en_batch = en_train[i : i + batch_size]\n",
    "            de_batch = de_train[i : i + batch_size]\n",
    "\n",
    "            # Numericalize and pad the *entire batch*\n",
    "            en_numericalized = [numericalize(text, en_sp) for text in en_batch]\n",
    "            de_numericalized = [numericalize(text, de_sp) for text in de_batch]\n",
    "\n",
    "            en_lengths = torch.tensor([tensor.shape[0] for tensor in en_numericalized])\n",
    "\n",
    "            en_input = nn.utils.rnn.pad_sequence(\n",
    "                en_numericalized, padding_value=en_word_padding_idx, batch_first=True\n",
    "            )\n",
    "            de_input = nn.utils.rnn.pad_sequence(\n",
    "                de_numericalized, padding_value=de_word_padding_idx, batch_first=True\n",
    "            )\n",
    "\n",
    "            en_input = en_input.to(device)\n",
    "            de_input = de_input.to(device)\n",
    "            en_lengths = en_lengths.to(device)\n",
    "\n",
    "            en_encoded, en_remap, _ = model.en_encoder(en_input, en_lengths)\n",
    "\n",
    "            # Decoder training (English to German) using Teacher Forcing\n",
    "            model.de_decoder.init_state(\n",
    "                None, en_encoded, en_remap\n",
    "            )  # Use English encoding to initialize German decoder\n",
    "\n",
    "            de_target_input = de_input[:, :-1].to(\n",
    "                device\n",
    "            )  # Shift target for teacher forcing\n",
    "            de_target_output = de_input[:, 1:].to(device)\n",
    "\n",
    "            de_decoded_output, _ = model.de_decoder(\n",
    "                de_target_input, en_encoded, memory_lengths=en_lengths\n",
    "            )  # Use English encoded output as memory bank\n",
    "            output = model.output_layer_de(de_decoded_output)\n",
    "\n",
    "            loss = criterion(\n",
    "                output.contiguous().view(-1, de_vocab_size),\n",
    "                de_target_output.contiguous().view(-1),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # losses.append((loss.item())) # DEBUG\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() / len(en_train)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Time: {epoch_time:.2f}s\"\n",
    "        )\n",
    "        # print(losses)\n",
    "        # exit(0)\n",
    "\n",
    "\n",
    "optimizer_en_de = optim.Adam(\n",
    "    list(model.parameters()),\n",
    "    lr=0.0001,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "train_en_to_de(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer_en_de,\n",
    "    en_sp,\n",
    "    de_sp,\n",
    "    num_epochs=20,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_en_to_de(\n",
    "    model: Model,\n",
    "    en_sp,\n",
    "    de_sp,\n",
    "    en_input,\n",
    "):\n",
    "    \"\"\"Evaluate English to German translation.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        en_numericalized = numericalize(en_input, en_sp).unsqueeze(0).to(device)\n",
    "        en_length = torch.tensor([en_numericalized.shape[1]]).to(device)\n",
    "        en_input = nn.utils.rnn.pad_sequence(\n",
    "            en_numericalized, padding_value=en_word_padding_idx, batch_first=True\n",
    "        ).to(device)\n",
    "\n",
    "        en_encoded, en_remap, en_lengths_output = model.en_encoder(en_input, en_length)\n",
    "\n",
    "        model.de_decoder.init_state(None, en_encoded, en_remap)\n",
    "\n",
    "        de_decoded_words = []\n",
    "        de_prev_word = torch.tensor([[de_sp.bos_id()]]).to(device)\n",
    "\n",
    "        for _ in range(en_length + 5):  # Max output length\n",
    "            de_decoder_output, _ = model.de_decoder(\n",
    "                de_prev_word, en_encoded, memory_lengths=en_lengths_output\n",
    "            )\n",
    "\n",
    "            output = model.output_layer_de(de_decoder_output)\n",
    "            de_predicted_word = output.argmax(2).squeeze()\n",
    "\n",
    "            if de_predicted_word.item() == de_sp.eos_id():\n",
    "                break\n",
    "\n",
    "            # de_decoded_words.append(\n",
    "            #     list(de_vocab.keys())[\n",
    "            #         list(de_vocab.values()).index(de_predicted_word.item())\n",
    "            #     ]\n",
    "            # )\n",
    "            de_decoded_words.append(de_sp.IdToPiece(de_predicted_word.item()))\n",
    "\n",
    "            de_prev_word = de_predicted_word.view(1, 1)\n",
    "\n",
    "    model.train()\n",
    "    return \"\".join(de_decoded_words).replace(\"▁\", \" \")\n",
    "\n",
    "\n",
    "en_input_sentence = \"A little girl climbing into a wooden playhouse.\"\n",
    "translated_sentence_de = evaluate_en_to_de(\n",
    "    model,\n",
    "    en_sp,\n",
    "    de_sp,\n",
    "    en_input_sentence,\n",
    ")\n",
    "print(f\"Translated Sentence (German): {translated_sentence_de}\")\n",
    "\n",
    "torch.save(model, \"model_20.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
