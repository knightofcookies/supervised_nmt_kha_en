{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49774246\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DIR = \"../datasets/samanatar\"\n",
    "lines = []\n",
    "\n",
    "for file in os.listdir(DIR):\n",
    "    with open(f\"{DIR}/{file}\", \"r\", encoding=\"utf-8\") as fp:\n",
    "        lines += fp.readlines()\n",
    "    \n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16729102\n"
     ]
    }
   ],
   "source": [
    "unique_lines = list(set(lines))\n",
    "del lines\n",
    "\n",
    "print(len(unique_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lines = sorted(unique_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\", 3: \"UNK\"}  # Add UNK token\n",
    "        self.n_words = 4  # Start with 4 to include UNK\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.add_word(word.lower())\n",
    "\n",
    "    def add_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word not in self.word2count:\n",
    "            self.word2count[word] = 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def build_vocab(self, max_vocab_size=1e8):\n",
    "        if max_vocab_size <= self.n_words:\n",
    "            raise ValueError(\"max_vocab_size must be greater than the number of existing tokens\")\n",
    "\n",
    "        # Sort words by frequency in descending order\n",
    "        sorted_words = sorted(self.word2count, key=self.word2count.get, reverse=True)\n",
    "\n",
    "        # Add most frequent words to vocabulary\n",
    "        if max_vocab_size < len(sorted_words):\n",
    "            limit  = max_vocab_size - self.n_words\n",
    "        else:\n",
    "            limit = len(sorted_words)\n",
    "        for word in sorted_words[:limit]:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def filter_sentence(sentence):\n",
    "    pattern = r\"[^A-Za-z0-9\\s\\t,;.:-?!'\\\"]\"\n",
    "\n",
    "    if sentence is None:\n",
    "        return False\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    if len(sentence) == 0:\n",
    "        return False\n",
    "    \n",
    "    if re.search(pattern, sentence):\n",
    "        return False\n",
    "    \n",
    "    word_count = len(sentence.split(' '))\n",
    "    if word_count < 5 or word_count > 30:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def normalize_lines(lines):\n",
    "    fl = []\n",
    "    for s in lines:\n",
    "        if filter_sentence(s):\n",
    "            s = unicode_to_ascii(str(s).strip())\n",
    "            s = re.sub(r\"(['\\\".!?:;,-])\", r\" \\1 \", s)\n",
    "            s = re.sub(r\"\\s+\", \" \", s)\n",
    "            # s = re.sub(r\"[^a-zA-Z0-9.,!?]+\", r\" \", s)\n",
    "            fl.append(s.strip())\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(unique_lines):\n",
    "    en_lang = Lang(\"english\")\n",
    "\n",
    "    unique_lines = normalize_lines(unique_lines)\n",
    "\n",
    "    print(\"Read %s sentences\" % len(unique_lines))\n",
    "    print(\"Counting words...\")\n",
    "    for en_line in unique_lines:\n",
    "        en_lang.add_sentence(en_line)\n",
    "\n",
    "    en_lang.build_vocab()\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(en_lang.name, en_lang.n_words)\n",
    "    return (en_lang, unique_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_word_frequencies(lang, title):\n",
    "    frequencies = list(lang.word2count.values())\n",
    "\n",
    "    # Histogram with log scale\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(frequencies, bins=np.logspace(0, np.log10(max(frequencies)), 50))\n",
    "    plt.xscale('log')\n",
    "    plt.title(f\"Word Frequency Distribution - {title}\")\n",
    "    plt.xlabel(\"Frequency (Log Scale)\")\n",
    "    plt.ylabel(\"Number of Words\")\n",
    "\n",
    "    # CDF\n",
    "    from statsmodels.distributions.empirical_distribution import ECDF\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ecdf = ECDF(frequencies)\n",
    "    plt.plot(ecdf.x, ecdf.y)\n",
    "    plt.xscale('log')\n",
    "    plt.title(f\"Word Frequency Distribution (CDF) - {title}\")\n",
    "    plt.xlabel(\"Frequency (Log Scale)\")\n",
    "    plt.ylabel(\"Cumulative Probability\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(\"samanatar/en_lang\"):\n",
    "        en_lang, en_lines = prepare_data(unique_lines=unique_lines)\n",
    "    else:\n",
    "        with open(\"samanatar/en_lang\", \"rb\") as fp:\n",
    "            en_lang = pickle.load(fp)\n",
    "\n",
    "    nel = []\n",
    "    for line in en_lines:\n",
    "        nel.append(line + \"\\n\")\n",
    "    with open(\"en_lines.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.writelines(nel)\n",
    "\n",
    "    del nel\n",
    "\n",
    "    plot_word_frequencies(en_lang, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"samanatar/en_lang\", \"ab\") as fp:\n",
    "    pickle.dump(en_lang, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loi = []\n",
    "ol = []\n",
    "for line in en_lines:\n",
    "    flag = False\n",
    "    words = line.split(' ')\n",
    "    for word in words:\n",
    "        if en_lang.word2count[word.lower()] <= 100:\n",
    "            loi.append(line + \"\\n\")\n",
    "            flag = True\n",
    "            break\n",
    "    if not flag:\n",
    "        ol.append(line + \"\\n\")\n",
    "\n",
    "with open(\"loi.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.writelines(loi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ol.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.writelines(ol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert (len(en_lines) == len(ol) + len(loi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8706799, 2337714, 11044513)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ol), len(loi), len(en_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
